---
title: "Final Project"
author: "Olusanya Oluwole"
date: "10/29/2021"
output: 
  html_document:
    toc: true
    toc_float: true
---

# Introduction
On daily basis, we produced and encountered huge text data in spoken or written 
forms and different languages. However, the only language computer understand 
is numbers. So, to be efficient, we need to train computers to understand spoken 
and written words. This can be achieved through Natural language processing ([NLP](https://www.ibm.com/cloud/learn/natural-language-processing)). NLP gives
computers ability to understand written text and spoken words in much the same
way human beings can. It enables computers to process human language the form of
text or voice data and to ‘understand’ its full meaning, complete with the 
speaker or writer’s intent and sentiment.

# Aims

1. To understand the basic relationships observed in the given 
data in order to interpret it more efficiently. 
2. To explore each data file, `en_us.blog`, `en_us.new` and `en_us.twitter`,
for *file sizes, number of characters, words and lines*.
3. To take samples of each file and combine them for further analysis
4. To examing word distribution in each files using table, histogram and word cloud. 
5. To build $N-grams$ of words to show relationship between words in the sample
dataset
6. To summarize word distribution and relationship in $N-gram$ with histograms
and wordclouds.
7. To build backoff predictive model for next word prediction.

# Useful Packages 
```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(tidytext, warn.conflicts = FALSE)
library(tidyverse, warn.conflicts = FALSE)
library(stringi, warn.conflicts = FALSE)
library(plotly, warn.conflicts = FALSE)
library(qdapRegex, warn.conflicts = FALSE)
library(wordcloud, warn.conflicts = FALSE)
library(RColorBrewer, warn.conflicts = FALSE)
library(syuzhet, warn.conflicts = FALSE)
library(SentimentAnalysis, warn.conflicts = FALSE)
library(sentimentr, warn.conflicts = FALSE)
library(data.table, warn.conflicts = FALSE)
```


# Basic Data Exploration

## Data Import
The three data file will be imported and sample taken for further analysis. We 
will remove profane word by filtering our data with words in the profanity_txt
file.

1. en_us.blogs
2. en_us.news
3. en_us.twitter
4. Profane words

```{r, warning=FALSE, message=FALSE}
setwd("C:/Users/justi/Documents/Olu_Drive/Coursera/Data_Science_Statistics_and_Machine_Learning_Specialization/Capstone/en_US")
blogs_txt <- readLines("en_US.blogs.txt", warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
news_txt <- readLines("en_US.news.txt", warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
twitter_txt <- readLines("en_US.twitter.txt", warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
profanity_txt <- readLines("profanity.txt", warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
profanity_df <- tibble(profanity_txt)
special_txt <- readLines("special.txt", warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
special_df <- tibble(special_txt)
stopWords_txt <- readLines("stopwords.txt", warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
stopWords_df <- tibble(stopWords_txt)
```

## Brief Data Summary
Here we determined basic features of the data `en_us.blog`, `en_us.new` and
`en_us.twitter`. The table below shows file sizes, number of characters (words,
spaces and others), words and lines of each data. 

```{r,echo=FALSE,warning=FALSE,message=FALSE,comment=NA, align='center', table.align = 'center'}

setwd("C:/Users/justi/Documents/Olu_Drive/Coursera/Data_Science_Statistics_and_Machine_Learning_Specialization/Capstone/en_US") # Set directory to the data
blogFileSize <- file.info("en_US.blogs.txt")$size / 1024^2 # i.e., 1024/1024
newsFileSize <- file.info("en_US.news.txt")$size  / 1024^2
twitterFileSize <- file.info("en_US.twitter.txt")$size / 1024^2


nCharsBlogs <- sum(nchar(blogs_txt))
nCharsNews <- sum(nchar(news_txt))
nCharsTwitter <- sum(nchar(twitter_txt))
nWordsBlogs <- sum(stri_count_words(blogs_txt)) # requires library(stringi)
nWordsNews <- sum(stri_count_words(news_txt))  
nWordsTwitter <- sum(stri_count_words(twitter_txt)) 
nLinesBlogs <- length(blogs_txt)
nLinesNews <- length(news_txt)  
nLinesTwitter <- length(twitter_txt) 

# Dataframe for file sizes, number of characters, words and lines
features_df <- tibble(
  "File Type" = c("Blogs", "News", "Twitter"),
  "File Size" = round(c(blogFileSize, newsFileSize, twitterFileSize), 2),
  "Number of Characters" = c(nCharsBlogs, nCharsNews, nCharsTwitter),
  "Number of Words" = c(nWordsBlogs, nWordsNews, nWordsTwitter),
  "Number of Lines" = c(nLinesBlogs, nLinesNews, nLinesTwitter)
  
)
features_df <- remove_rownames(features_df)
knitr::kable(features_df, format = "markdown", caption = "Data Properties",
             align = "c")

```

## Sampling
I will sample 0.5% of each data set `blogs_txt`, `news_txt` and `twitter_txt` to
form a single set, `sample1_txt`. See below the first 3 lines of the `sample-txt`
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
set.seed(12345)
blogsSample_txt    <- sample(blogs_txt, length(blogs_txt) * 0.005, replace = FALSE)
newsSample_txt     <- sample(news_txt, length(news_txt) * 0.005, replace = FALSE)
twitterSample_txt  <- sample(twitter_txt, length(twitter_txt) * 0.005, replace = FALSE)
sample1_txt = c(blogsSample_txt, newsSample_txt, twitterSample_txt)
head(sample1_txt, 3)
```

# Data Preprocessing
First, we need to clean the data and remove irrelevant characters so we can 
concentrate on the important words from this file. 

1. Remove lines with latin1 and ASCII characters.
```{r, warning=FALSE, message=FALSE}
latin1ASII_func <- grep("latin1ASII", iconv(sample1_txt, "latin1", "ASCII", sub="latin1ASII"))
sample2_txt <- sample1_txt[-latin1ASII_func]
```

2. Remove special characters, digits and extra white space.

See below the first 5 lines of the clean set.
```{r, warning=FALSE, message=FALSE}
sample3_txt <- gsub("&amp", " ", sample2_txt)
sample3_txt <- gsub("RT :|@[a-z,A-Z]*: ", " ", sample3_txt) # remove tweets
sample3_txt <- gsub("@\\w+", " ", sample3_txt)
sample3_txt <- gsub("[[:digit:]]", " ", sample3_txt) # remove digits
sample3_txt <- gsub(" #\\S*"," ", sample3_txt)  # remove hash tags 
sample3_txt <- gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", sample3_txt) # remove url
sample3_txt <- gsub("[^[:alnum:][:space:]']", "", sample3_txt) # Remove punctuation except apostrophes
sample3_txt <- rm_white(sample3_txt) # remove extra spaces using `qdapRegex` package
```

See below the first 5 lines of the clean set.
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
# Create sample set dataframe (sample_df)
sample_df <- tibble(line = 1:length(sample3_txt), text = sample3_txt)
head(sample_df$text, 3)
```

## Tokenization
A token is a meaningful unit of text, most often a word, that we are 
interested in using for further analysis, and tokenization is the process of
splitting text into tokens.

### Create one-token-per-document-per-row
We need to both break the text into individual tokens and transform it to 
a tidy data structure. This equivalent to a unigram $1-gram$. Also, we need to
filter out the profane word from the text corpus.
```{r, warning=FALSE, message=FALSE}
sample_df2 <- sample_df %>%
  unnest_tokens(word, text) %>% 
  filter(!word %in% profanity_df$profanity_txt) %>% 
  filter(!word %in% special_df$special_txt) %>% # Remove profane words
  drop_na()

```

## Find most frequent words
The `count()` will will be useful here. This will help use to visualize the
dataset. See below five most frequent word and their frequencies $n$.
```{r, warning=FALSE, message=FALSE, comment=NA}
unigram <- sample_df2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = reorder(word, n)) %>% 
  filter(n > 10)
head(unigram, 5)
```

# Data Visualization of the data

## Create histogram
We use `ggplot` to generate the histogram and line graph below. Word occurrence
is more than 800 times.
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
unigramPlot <- unigram %>% 
  filter(n > 800) %>% 
  ggplot() +
  aes(n, word) +
  geom_col(stat = "identity", color="darkgray", fill="chartreuse2") +
  labs(x = "Frequency", y = "Word")
ggplotly(unigramPlot)
```

## Line graph
```{r, echo=FALSE, warning=FALSE, message=FALSE}
unigramLine <- unigram %>% 
  filter(n > 800) %>% 
  ggplot() +
  aes(n, word) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.3, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(x = "Frequency", y = "Word")
ggplotly(unigramLine)
```

## Word cloud
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
unigram %>%
with(wordcloud(word, n, max.words = 500, colors = brewer.pal(8, "Dark2")))
```
# Sentiment and Emotion Analysis
[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is use 
to systematically identify, extract, quantify, and study affective states and 
subjective information from text data. It help to understand the social 
sentiment in a data. while [Emotion analysis](https://www.bytesview.com/emotion-analysis)
identify and analyze the underlying emotions expressed in the data such as good 
or bad, sad or happy etc.

## Sentiment Analysis
The pie chart show that most of the sentiments expressed in the sample text are
positive.
```{}
r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}

sentiment_txt <- sample_df2 %>% 
  filter(!word %in% profanity_df$profanity_txt) %>%
  filter(!word %in% special_df$special_txt) %>%
  anti_join(stop_words)

sentiment_txt <- sentiment_txt$word
  
  
sentiment_df <- analyzeSentiment(sentiment_txt)

# Save data to r object
saveRDS(sentiment_df, "sentiment_df.rds")

# Extract dictionary-based sentiment according to the QDAP dictionary
SentimentQDAP_df <- sentiment_df$SentimentQDAP

# View sentiment direction (i.e. positive, neutral and negative)
sentimentDirection_char <- convertToDirection(SentimentQDAP_df)
sentimentDirection_df <- data.frame("SentimentDirection" = sentimentDirection_char)

# Combine sentiment direction with SentimentQDAP in a data set
sentimentDirection_df$SentimentQDAP <- sentiment_df$SentimentQDAP

# Draw a pie chart
sentimentDirection_df %>% 
  drop_na() %>% 
  ggplot(., aes(x = "", y = SentimentDirection, fill = SentimentDirection)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  theme_void()

```

## Emotion Analysis
The histogram below reveal the emotion expressed in the data set.
```{}
r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA, results=FALSE}

emotion_txt <- sample_df2 %>% 
  filter(!word %in% profanity_df$profanity_txt) %>%
  filter(!word %in% special_df$special_txt) %>%
  anti_join(stop_words)

emotion_txt <- emotion_txt$word

emotion_df <- setDF(emotion_by(get_sentences(emotion_txt)))

# Save data to r object
saveRDS(emotion_df, "emotion_df.rds")

emotion_df$emotionType <- as.character(emotion_df$emotion_type)
emotion_df2 <- emotion_df %>% 
  select(!emotion_type) %>% 
  filter(!emotionType %in% c("anticipation_negated", "fear_negated",
                             "surprise_negated", "disgust_negated",
                             "sadness_negated", "joy_negated", "trust_negated",
                             "anger_negated"))
# Histogram
ggplot(emotion_df2) +
  aes(reorder(emotionType, emotion_count), emotion_count, fill = emotionType) +
  geom_histogram(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip() +
  labs(x = "Emotion", y = "Frequency")
```

# N-Grams Analysis
Essentially, the step under **Tokenization** above is equivalent to $1-gram$. 
From here, we visualize the data inform of $2-gram$, $3-gram$ and $4-gram$.

## Bigram

### Creating Bigram 
Generate $2-gram$ token and remove profane words.
```{r, warning=FALSE, message=FALSE}
bigram <- sample_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ", 
           extra = "drop", fill = "right") %>%
  filter(!word1 %in% profanity_df$profanity_txt,
         !word2 %in% profanity_df$profanity_txt,
         !word1 %in% special_df$special_txt,
         !word2 %in% special_df$special_txt) %>% # Remove profane words
  drop_na() %>% 
  unite(bigram, word1, word2, sep = " ")
```


### Find Most Frequent Words in the Bigram
See below five most frequent bigram and their frequencies $n$.
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
bigram <- bigram %>% 
  count(bigram, sort = TRUE) %>% 
  mutate(bigram = reorder(bigram, n)) %>%
  filter(n > 10) 
head(bigram, 5)
```

### Bigram visualization
The histogram and line graph show $2-gram$ with occurrence of more than 200 times.

#### Histogram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA, fig.align = "center"}
bigramPlot <- ggplot(subset(bigram, n > 200)) +
  aes(bigram, n) +
  geom_histogram(stat = "identity", color="darkgray", fill="blueviolet") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip() +
  labs(x = "Bigram", y = "Frequency")
ggplotly(bigramPlot)
```

#### Line Graph for Bigram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
bigramLine <- bigram %>% 
  filter(n > 200) %>% 
  ggplot() +
  aes(n, bigram) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.3, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = bigram), check_overlap = TRUE, vjust = 1.5) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(x = "Frequency", y = "Bigram")
ggplotly(bigramLine)
```

#### Word cloud for Bigram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
bigram %>%
with(wordcloud(bigram, n, max.words = 200, colors = brewer.pal(8, "Dark2")))
```

## Trigram

### Creating Trigram 

Generate $3-gram$ token and remove profane words
```{r, warning=FALSE, message=FALSE}
trigram <- sample_df %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ", 
           extra = "drop", fill = "right") %>%
  filter(!word1 %in% profanity_df$profanity_txt,
         !word2 %in% profanity_df$profanity_txt,
         !word3 %in% profanity_df$profanity_txt,
         !word1 %in% special_df$special_txt,
         !word2 %in% special_df$special_txt,
         !word3 %in% special_df$special_txt) %>% # Remove profane words
  drop_na() %>% 
  unite(trigram, word1, word2, word3, sep = " ")
```


### Find Most Frequent Words in the Trigram
See below five most frequent trigram and their frequencies $n$.
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
trigram <- trigram %>% 
  count(trigram, sort = TRUE) %>%
  mutate(trigram = reorder(trigram, n)) %>%
  filter(n > 10)
head(trigram, 5)
```

### Trigram visualization
The histogram and line graph show $3-gram$ with occurrence of more than 40 times.

#### Histogram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
trigramPlot <- ggplot(subset(trigram, n > 40)) +
  aes(x = trigram, y = n) +
  geom_histogram(stat = "identity", color="darkgray", fill="forestgreen") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip() +
  labs(x = "Trigram", y = "Frequency")
ggplotly(trigramPlot)
```

#### Line Graph for Trigram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
trigramLine <- trigram %>% 
  filter(n > 40) %>% 
  ggplot() +
  aes(y=trigram, x=n) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.3, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = trigram), check_overlap = TRUE, vjust = 1.5) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(x = "Frequency", y = "Trigram")
ggplotly(trigramLine)
```

#### Word cloud for Trigram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
trigram %>%
with(wordcloud(trigram, n, max.words = 50, colors = brewer.pal(8, "Dark2")))
```

## Quadgram

### Creating Quadgram 
Generate $4-gram$ token and remove profane words.
```{r, warning=FALSE, message=FALSE}
quadgram <- sample_df %>%
  unnest_tokens(quadgram, text, token = "ngrams", n = 4) %>% 
  separate(quadgram, c("word1", "word2", "word3", "word4"), sep = " ", 
           extra = "drop", fill = "right") %>%
  filter(!word1 %in% profanity_df$profanity_txt,
         !word2 %in% profanity_df$profanity_txt,
         !word3 %in% profanity_df$profanity_txt,
         !word4 %in% profanity_df$profanity_txt,
         !word1 %in% special_df$special_txt,
         !word2 %in% special_df$special_txt,
         !word3 %in% special_df$special_txt,
         !word4 %in% special_df$special_txt) %>% # Remove profane words
  drop_na() %>% 
  unite(quadgram, word1, word2, word3, word4, sep = " ")
```


### Find Most Frequent Words in the Quadigram
See below five most frequent quadgram and their frequencies $n$.
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
quadgram <- quadgram %>% 
  count(quadgram, sort = TRUE) %>% 
  mutate(quadgram = reorder(quadgram, n)) %>%
  filter(n > 5)
head(quadgram, 5)
```

### Quadigram visualization
The histogram and line graph show $3-gram$ with occurrence of more than 10 times.

#### Histogram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
quadgramPlot <- ggplot(subset(quadgram, n > 10)) +
  aes(quadgram, n) +
  geom_histogram(stat = "identity", color="darkgray", fill="coral2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip() +
  labs(x = "Quadgram", y = "Frequency")
ggplotly(quadgramPlot)
```

#### Line Graph for Quadigram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
quadgramLine <- quadgram %>% 
  filter(n > 10) %>% 
  ggplot() +
  aes(y=quadgram, x=n) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.3, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = quadgram), check_overlap = TRUE, vjust = 1.5) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(x = "Frequency", y = "Quadgram")
ggplotly(quadgramLine)
```

#### Word cloud for Quadigram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
quadgram %>%
with(wordcloud(quadgram, n, max.words = 50, colors = brewer.pal(8, "Dark2")))
```
## Quintgram

### Creating Quintgram 
Generate $5-gram$ token and remove profane words.
```{r, warning=FALSE, message=FALSE}
quintgram <- sample_df %>%
  unnest_tokens(quintgram, text, token = "ngrams", n = 5) %>% 
  separate(quintgram, c("word1", "word2", "word3", "word4", "word5"), sep = " ", 
           extra = "drop", fill = "right") %>%
  filter(!word1 %in% profanity_df$profanity_txt,
         !word2 %in% profanity_df$profanity_txt,
         !word3 %in% profanity_df$profanity_txt,
         !word4 %in% profanity_df$profanity_txt,
         !word5 %in% profanity_df$profanity_txt,
         !word1 %in% special_df$special_txt,
         !word2 %in% special_df$special_txt,
         !word3 %in% special_df$special_txt,
         !word4 %in% special_df$special_txt,
         !word5 %in% special_df$special_txt) %>% # Remove profane words
  drop_na() %>% 
  unite(quintgram, word1, word2, word3, word4, word5, sep = " ")
```

### Find Most Frequent Words in the Quintgram
See below five most frequent quadgram and their frequencies $n$.
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
quintgram <- quintgram %>% 
  count(quintgram, sort = TRUE)%>%
  mutate(quintgram = reorder(quintgram, n)) %>%
  filter(n > 3) 
head(quintgram, 5)
```

#### Histogram
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
quintgramPlot <- ggplot(subset(quintgram, n > 5)) +
  aes(quintgram, n) +
  geom_histogram(stat = "identity", color="darkgray", fill="coral2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip() +
  labs(x = "Quintgram", y = "Frequency")
ggplotly(quintgramPlot)
```


# Modeling and Prediction
We will use $2-gram$, $3-grams) and $4-grams$ to build the required model for 
next word prediction. 

## Building the n-grams database
Split each $N-gram$ into the constituent words and store them back into the same
data frame

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
# Bigram
bigram_df <- bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ", 
           extra = "drop", fill = "right") %>%
  drop_na() 
bigram_df$bigram <- bigram$bigram

# Trigram
trigram_df <- trigram %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ", 
           extra = "drop", fill = "right") %>%
  drop_na() 
trigram_df$trigram <- trigram$trigram

# Quadgram
quadgram_df <- quadgram %>%
  separate(quadgram, c("word1", "word2", "word3", "word4"), sep = " ", 
           extra = "drop", fill = "right") %>%
  drop_na() 
quadgram_df$quadgram <- quadgram$quadgram
```

## Simple prediction model using $2-gram$.
Find next word for "good". 

Filter data frame where first word is "good" to find the next possible words.
```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
nextword_filtered = bigram_df[
  bigram_df$word1 == "good", 
  c("n", "word2")]

#Order in descending order of frequency
nextword_ordered = nextword_filtered[
  with(nextword_filtered, order(-n)), ]

#The predicted next words
head(nextword_ordered$word2, 4)
```
The next possible words are as shown above. We will build a more reliable model
going forward.


```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA, eval=FALSE}
# Save the data for the Next Word Predictor Shiny App
#dir.create("final_project_ngram_data", showWarnings = FALSE)

saveRDS(unigram, "unigram.rds")
saveRDS(bigram_df, "bigram.rds")
saveRDS(trigram_df,"trigram.rds")
saveRDS(quadgram_df,"quadgram.rds")
#saveRDS(quintgram_df,"quintgram.rds")
```

## Building prediction model
First, we will creating matching functions for each $N-grams$
```{r, warning=FALSE, message=FALSE, comment=NA}
# Bigram matching
bigram_func <- function(inputWords){
  num <- length(inputWords)
  # Number of rows to be selected
  nRow <- 1L
  filter(bigram_df, word1==inputWords[num]) %>%
    add_count(word2, sort = TRUE) %>%
    top_n(3, n) %>% 
    filter(row_number() == nRow) %>%
    select(num_range("word", 2)) %>%
    as.character() -> out
  ifelse(out =="character(0)", "?", return(out))
}

# Trigram matching
trigram_func <- function(inputWords){
  num <- length(inputWords)
  # Number of rows to be selected
  nRow <- 1L
  filter(trigram_df,
         word1==inputWords[num-1],
         word2==inputWords[num])  %>%
    add_count(word3, sort = TRUE) %>%
    top_n(3, n) %>%
    filter(row_number() == nRow) %>%
    select(num_range("word", 3)) %>%
    as.character() -> out
  ifelse(out=="character(0)", bigram_func(inputWords), return(out))
}

# Quadgram matching
quadgram_func <- function(inputWords){
  num <- length(inputWords)
  # Number of rows to be selected
  nRow <- 1L
  filter(quadgram_df,
         word1==inputWords[num-2],
         word2==inputWords[num-1],
         word3==inputWords[num])  %>%
    add_count(word4, sort = TRUE) %>%
    top_n(3, n) %>%
    filter(row_number() == nRow) %>%
    select(num_range("word", 4)) %>%
    as.character() -> out
  ifelse(out=="character(0)", trigram_func(inputWords), return(out))
}
```

### Next Word Prediction Function
This function will be use to predict next word when a word or phrase is entered.
```{r, warning=FALSE, message=FALSE, comment=NA}
ngrams_func <- function(wordPhraseInput){
  # Create a dataframe
  wordPhraseInput <- data.frame(text = wordPhraseInput)
  # Clean the Inpput
  replace_reg <- "[^[:alpha:][:space:]]*"
  wordPhraseInput <- wordPhraseInput %>%
    mutate(text = str_replace_all(text, replace_reg, ""))
  # Find word count, separate words, lower case
  inputCount <- str_count(wordPhraseInput, boundary("word"))
  inputWords <- unlist(str_split(wordPhraseInput, boundary("word")))
  inputWords <- tolower(inputWords)
  # Call the matching functions
  out <- ifelse(inputCount == 1, bigram_func(inputWords), 
                ifelse(inputCount == 2, trigram_func(inputWords),
                       quadgram_func(inputWords)))
  # Output
  return(out)
}
```


### Predict next word
Predict next word for the following words and phrases.

1. *happy*
3. *my new*
3. *good to see*
4. *just to let you*
5. *thank you so*
```{r, warning=FALSE, message=FALSE, comment=NA}
ngrams_func("happy")
ngrams_func("my new")
ngrams_func("good to see")
ngrams_func("just to let you")
ngrams_func("thank you so")
```















